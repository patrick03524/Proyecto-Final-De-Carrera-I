# -*- coding: utf-8 -*-
"""Image Inpainting for Irregular Holes Using Partial Convolutions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DXz8_6HwspqX_Ky6O4ufjo37TM7KNZJX

# Replica del artículo científico "*Image Inpainting for Irregular Holes Using Partial Convolutions*"
## Alumno: Patrick Xavier Marquez Choque
## Curso: Proyecto Final de Carrera I
## Periodo: 2022-I

# Paso 1: Librerías Necesarias
"""

import os
import sys
import numpy as np
import cv2
from datetime import datetime

##  Plotting Images
import matplotlib
import matplotlib.pyplot as plt
import itertools
from copy import deepcopy

##  Images
import imageio
from skimage import io
from PIL import Image
from random import randint
from random import seed
from torchvision import transforms, utils

## Our Masking Class
#Dataset
from mpl_toolkits.axes_grid1 import ImageGrid
from tensorflow import keras
from tensorflow.keras import backend as K
from tensorflow.keras.layers import InputSpec
from tensorflow.keras.layers import Conv2D

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi
!/usr/local/cuda/bin/nvcc --version

"""# Paso 2: Definición del Dataset a utilizar Cifar10"""

def cifar10():
    print('Loading CIFAR-10 dataset')
    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
    print('x_train shape:', x_train.shape)
    print(x_train.shape[0], 'train samples')
    print(x_test.shape[0], 'test samples')
    return (x_train, y_train), (x_test, y_test)

def previewDataset(images,labels,numbers=32):
    sample_images = images[:numbers]
    sample_labels = labels[:numbers]

    fig = plt.figure(figsize=(16., 8.))
    grid = ImageGrid(fig, 111, nrows_ncols=(4, 8), axes_pad=0.3)

    for ax, image, label in zip(grid, sample_images, sample_labels):
        ax.imshow(image)
        ax.set_title(label[0])
    plt.show()


def previewGeneratorData(generator):
   nSamples=90
   [maskedImages, masks], originalImages = generator[nSamples]
   previewImage = [None]*(len(maskedImages)+len(masks)+len(originalImages))
   
   previewImage[::3] = originalImages
   previewImage[1::3] = masks
   previewImage[2::3] = maskedImages

   fig = plt.figure(figsize=(17., 8.))
   grid = ImageGrid(fig, 111, nrows_ncols=(4, 9), axes_pad=0.3)

   for ax, image in zip(grid, previewImage):
      ax.imshow(image)

   plt.show()

"""# Paso 3: Definción de etapa de enmascaramiento para el conjunto de datos"""

def maskImage(img, dim = (32,32,3)):
      ## Prepare masking matrix
    mask = np.full(dim, 255, np.uint8) ## White background
    for _ in range(np.random.randint(1, 10)):
    # Get random x locations to start line
        x1, x2 = np.random.randint(1, dim[0]), np.random.randint(1, dim[0])
        # Get random y locations to start line
        y1, y2 = np.random.randint(1, dim[1]), np.random.randint(1, dim[1])
        # Get random thickness of the line drawn
        thickness = np.random.randint(1, dim[2])
        # Draw black line on the white mask
        cv2.line(mask,(x1,y1),(x2,y2),(0,0,0),thickness)

    ## Mask the image
    masked_image = img.copy()
    masked_image[mask==0] = 255

    return masked_image, mask

"""# Paso 4: Definición de la Clase generadora de nuestro modelo"""

class DataGenerator(keras.utils.Sequence):
   def __init__(self, X, y, batch_size=32, dim=(32, 32),
      n_channels=3, shuffle=True):
   
      self.batch_size = batch_size
      self.X = X
      self.y = y
      self.dim = dim
      self.n_channels = n_channels
      self.shuffle = shuffle
      self.on_epoch_end()

   def __len__(self):
      # Denotes the number of batches per epoch
      return int(np.floor(len(self.X) / self.batch_size))

   def __getitem__(self, index):
      # Generate one batch of data
      indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
      X_inputs, y_output = self.__data_generation(indexes)
      return X_inputs, y_output

   def on_epoch_end(self):
      # Updates indexes after each epoch
      self.indexes = np.arange(len(self.X))
      if self.shuffle:
         np.random.shuffle(self.indexes)
   
   def __data_generation(self, idxs):
      # Masked_images is a matrix of masked images used as input
      Masked_images = np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels)) # Masked image
      # Mask_batch is a matrix of binary masks used as input
      Mask_batch = np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels)) # Binary Masks
      # y_batch is a matrix of original images used for computing error from reconstructed image
      y_batch = np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels)) # Original image


      ## Iterate through random indexes
      for i, idx in enumerate(idxs):
         image_copy = self.X[idx].copy()

         ## Get mask associated to that image
         masked_image, mask = maskImage(image_copy)

         Masked_images[i,] = masked_image/255
         Mask_batch[i,] = mask/255
         y_batch[i] = self.y[idx]/255

      ## Return mask as well because partial convolution require the same.
      return [Masked_images, Mask_batch], y_batch

"""# Paso 5: Implementación del Modelo PConv"""

class PConv2D(Conv2D):
    def __init__(self, *args, n_channels=3, mono=False, **kwargs):
        super().__init__(*args, **kwargs)
        self.input_spec = [InputSpec(ndim=4), InputSpec(ndim=4)]

    def build(self, input_shape):        
        """Adapted from original _Conv() layer of Keras        
        param input_shape: list of dimensions for [img, mask]
        """
        
        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1
            
        if input_shape[0][channel_axis] is None:
            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')
            
        self.input_dim = input_shape[0][channel_axis]
        
        # Image kernel
        kernel_shape = self.kernel_size + (self.input_dim, self.filters)
        self.kernel = self.add_weight(shape=kernel_shape,
                                      initializer=self.kernel_initializer,
                                      name='img_kernel',
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        # Mask kernel
        self.kernel_mask = K.ones(shape=self.kernel_size + (self.input_dim, self.filters))

        # Calculate padding size to achieve zero-padding
        self.pconv_padding = (
            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), 
            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), 
        )

        # Window size - used for normalization
        self.window_size = self.kernel_size[0] * self.kernel_size[1]
        
        if self.use_bias:
            self.bias = self.add_weight(shape=(self.filters,),
                                        initializer=self.bias_initializer,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None
        self.built = True

    def call(self, inputs, mask=None):
        '''
        We will be using the Keras conv2d method, and essentially we have
        to do here is multiply the mask with the input X, before we apply the
        convolutions. For the mask itself, we apply convolutions with all weights
        set to 1.
        Subsequently, we clip mask values to between 0 and 1
        ''' 

        # Both image and mask must be supplied
        if type(inputs) is not list or len(inputs) != 2:
            raise Exception('PartialConvolution2D must be called on a list of two tensors [img, mask]. Instead got: ' + str(inputs))

        # Padding done explicitly so that padding becomes part of the masked partial convolution
        images = K.spatial_2d_padding(inputs[0], self.pconv_padding, self.data_format)
        masks = K.spatial_2d_padding(inputs[1], self.pconv_padding, self.data_format)

        # Apply convolutions to mask
        mask_output = K.conv2d(
            masks, self.kernel_mask, 
            strides=self.strides,
            padding='valid',
            data_format=self.data_format,
            dilation_rate=self.dilation_rate
        )

        # Apply convolutions to image
        img_output = K.conv2d(
            (images*masks), self.kernel, 
            strides=self.strides,
            padding='valid',
            data_format=self.data_format,
            dilation_rate=self.dilation_rate
        )        

        # Calculate the mask ratio on each pixel in the output mask
        mask_ratio = self.window_size / (mask_output + 1e-8)

        # Clip output to be between 0 and 1
        mask_output = K.clip(mask_output, 0, 1)

        # Remove ratio values where there are holes
        mask_ratio = mask_ratio * mask_output

        # Normalize iamge output
        img_output = img_output * mask_ratio

        # Apply bias only to the image (if chosen to do so)
        if self.use_bias:
            img_output = K.bias_add(
                img_output,
                self.bias,
                data_format=self.data_format)
        
        # Apply activations on the image
        if self.activation is not None:
            img_output = self.activation(img_output)
            
        return [img_output, mask_output]
    
    def compute_output_shape(self, input_shape):
        if self.data_format == 'channels_last':
            space = input_shape[0][1:-1]
            new_space = []
            for i in range(len(space)):
                new_dim = conv_output_length(
                    space[i],
                    self.kernel_size[i],
                    padding='same',
                    stride=self.strides[i],
                    dilation=self.dilation_rate[i])
                new_space.append(new_dim)
            new_shape = (input_shape[0][0],) + tuple(new_space) + (self.filters,)
            return [new_shape, new_shape]
        if self.data_format == 'channels_first':
            space = input_shape[2:]
            new_space = []
            for i in range(len(space)):
                new_dim = conv_output_length(
                    space[i],
                    self.kernel_size[i],
                    padding='same',
                    stride=self.strides[i],
                    dilation=self.dilation_rate[i])
                new_space.append(new_dim)
            new_shape = (input_shape[0], self.filters) + tuple(new_space)
            return [new_shape, new_shape]

def conv_output_length(input_length, filter_size,
                       padding, stride, dilation=1):
    """Determines output length of a convolution given input length.
    # Arguments
        input_length: integer.
        filter_size: integer.
        padding: one of `"same"`, `"valid"`, `"full"`.
        stride: integer.
        dilation: dilation rate, integer.
    # Returns
        The output length (integer).
    """
    if input_length is None:
        return None
    assert padding in {'same', 'valid', 'full', 'causal'}
    dilated_filter_size = (filter_size - 1) * dilation + 1
    if padding == 'same':
        output_length = input_length
    elif padding == 'valid':
        output_length = input_length - dilated_filter_size + 1
    elif padding == 'causal':
        output_length = input_length
    elif padding == 'full':
        output_length = input_length + dilated_filter_size - 1
    return (output_length + stride - 1) // stride

def dice_coef(y_true, y_pred):
    y_true_f = keras.backend.flatten(y_true)
    y_pred_f = keras.backend.flatten(y_pred)
    intersection = keras.backend.sum(y_true_f * y_pred_f)
    return (2. * intersection) / (keras.backend.sum(y_true_f + y_pred_f))

class PCNN:
  '''
  Build UNET like model
  '''
  def prepare_model(self, input_size=(32,32,3)):
    input_image = keras.layers.Input(input_size)
    input_mask = keras.layers.Input(input_size, name='encoder_input')
  
    conv1, mask1, conv2, mask2 = self.__encoder_layer(32, input_image, input_mask, ['conv1', 'conv2'])
    conv3, mask3, conv4, mask4 = self.__encoder_layer(64, conv2, mask2, ['conv3', 'conv4'])
    conv5, mask5, conv6, mask6 = self.__encoder_layer(128, conv4, mask4, ['conv5', 'conv6'])
    conv7, mask7, conv8, mask8 = self.__encoder_layer(256, conv6, mask6, ['conv7', 'encoder_output'])

    conv9, mask9, conv10, mask10 = self.__decoder_layer(256, 128, conv8, mask8, conv7, mask7, ['conv9', 'conv10'])
    conv11, mask11, conv12, mask12 = self.__decoder_layer(128, 64, conv10, mask10, conv5, mask5, ['conv11', 'conv12'])
    conv13, mask13, conv14, mask14 = self.__decoder_layer(64, 32, conv12, mask12, conv3, mask3, ['conv13', 'conv14'])
    conv15, mask15, conv16, mask16 = self.__decoder_layer(32, 3, conv14, mask14, conv1, mask1, ['conv15', 'decoder_output'])

    outputs = keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(conv16)

    return keras.models.Model(inputs=[input_image, input_mask], outputs=[outputs])
    
  def __encoder_layer(self, filters, in_layer, in_mask, names):
    conv1, mask1 = PConv2D(32, (3,3), strides=1, padding='same', name=names[0])([in_layer, in_mask])
    conv1 = keras.activations.relu(conv1)

    conv2, mask2 = PConv2D(32, (3,3), strides=2, padding='same', name=names[1])([conv1, mask1])
    conv2 = keras.activations.relu(conv2)

    return conv1, mask1, conv2, mask2

  def __decoder_layer(self, filter1, filter2, in_img, in_mask, share_img, share_mask, names):
    up_img = keras.layers.UpSampling2D(size=(2,2))(in_img)
    up_mask = keras.layers.UpSampling2D(size=(2,2))(in_mask)
    concat_img = keras.layers.Concatenate(axis=3)([share_img, up_img])
    concat_mask = keras.layers.Concatenate(axis=3)([share_mask, up_mask])

    conv1, mask1 = PConv2D(filter1, (3,3), padding='same', name=names[0])([concat_img, concat_mask])
    conv1 = keras.activations.relu(conv1)

    conv2, mask2 = PConv2D(filter2, (3,3), padding='same', name=names[1])([conv1, mask1])
    # conv2 = keras.layers.BatchNormalization()(conv2)
    conv2 = keras.activations.relu(conv2)

    return conv1, mask1, conv2, mask2

"""# Paso 6: Carga de nuestros datos para el entrenamiento del modelo"""

(x_train, y_train), (x_test, y_test) = cifar10()

trainGenerator = DataGenerator(x_train, x_train, shuffle=True)
testGenerator = DataGenerator(x_test, x_test, shuffle=False)

"""# Paso 7: Previsualización de nuestras máscaras"""

previewGeneratorData(trainGenerator)

"""# Paso 7: Entrenamiento del modelo"""

keras.backend.clear_session()
model = PCNN().prepare_model()
model.summary()
model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy']) #exactitud

_ = model.fit(trainGenerator, validation_data=testGenerator, 
          epochs=32, 
          steps_per_epoch=len(trainGenerator), 
          validation_steps=len(testGenerator),
          use_multiprocessing=True)

"""# Paso 8: Expermientación con los primeros elementos de nuestro conjunto de datos"""

testCases = 10

[masked_images, masks], sample_labels = testGenerator[54]
fig, axs = plt.subplots(nrows=testCases, ncols=4, figsize=(10, 2*testCases))
for i in range(testCases):
  inputs = [masked_images[i].reshape((1,)+masked_images[i].shape), masks[i].reshape((1,)+masks[i].shape)]
  impainted_image = model.predict(inputs)
  axs[i][0].imshow(masked_images[i])
  axs[i][1].imshow(masks[i])
  axs[i][2].imshow(impainted_image.reshape(impainted_image.shape[1:]))
  axs[i][3].imshow(sample_labels[i])
  
plt.show()
# Image With Mask || Mask || Generated Image || Original Image

"""# Paso 9: Métricas de Evaluación PSNR y SSIM"""

#PSNR
from skimage import io
import tensorflow as tf
#tf.compat.v1.disable_eager_execution()
from skimage.restoration import denoise_nl_means, estimate_sigma
from skimage import img_as_ubyte, img_as_float

def log10(x):
    numerator = tf.math.log(x)
    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))
    return numerator / denominator

def psnr(im1, im2):
    img_arr1 = np.array(im1).astype('float32')
    img_arr2 = np.array(im2).astype('float32')
    mse = tf.reduce_mean(tf.math.squared_difference(img_arr1, img_arr2))
    psnr = tf.constant(255**2, dtype=tf.float32)/mse
    result = tf.constant(10, dtype=tf.float32)*log10(psnr)
    with tf.compat.v1.Session():
        result = result.eval()
    return result
  
def psnr_image(img):
  float_img = img_as_float(img)
  sigma_est = np.mean(estimate_sigma(float_img, multichannel=True))
  denoise_img = denoise_nl_means(float_img, h=1.15 * sigma_est, fast_mode=False, 
                                patch_size=5, patch_distance=3, multichannel=True)
  denoise_img_as_8byte = img_as_ubyte(denoise_img)

  segm1 = (denoise_img_as_8byte <= 64)
  segm2 = (denoise_img_as_8byte > 64) & (denoise_img_as_8byte <= 97)
  segm3 = (denoise_img_as_8byte > 97) & (denoise_img_as_8byte <= 137)
  segm4 = (denoise_img_as_8byte > 137) & (denoise_img_as_8byte <= 179)
  segm5 = (denoise_img_as_8byte > 179)

  all_segments = np.zeros((denoise_img_as_8byte.shape[0], denoise_img_as_8byte.shape[1],3)) #nothing but denoise img size but blank

  all_segments[segm1] = 0.1
  all_segments[segm2] = 0.2
  all_segments[segm3] = 0.3
  all_segments[segm4] = 0.4
  all_segments[segm5] = 0.5

  return psnr(img, all_segments)

#SSIM
import imutils
from skimage import measure

def image_rotate(img, img_comparison,angle):
    img_comparison = imutils.rotate(img_comparison,angle)    
    grayA = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    grayB = cv2.cvtColor(img_comparison, cv2.COLOR_BGR2GRAY)       
    score, diff = measure.compare_ssim(grayA, grayB, full=True)
    
    return score

def image_rotate_resize(img, img_comparison,angle):
    size = (8,8)
    img_cut = cv2.resize(img,size)
    img_comparison_mid = cv2.resize(img_comparison,size)
    img_comparison_cut = imutils.rotate(img_comparison_mid,angle)
    grayA = cv2.cvtColor(img_cut, cv2.COLOR_BGR2GRAY)
    grayB = cv2.cvtColor(img_comparison_cut, cv2.COLOR_BGR2GRAY)   
    score, diff = measure.compare_ssim(grayA, grayB, full=True)
    
    return score

def deal_image(img, img_comparison):
    if img.shape[:2] == img_comparison.shape[:2]:
        score1  = image_rotate(img, img_comparison,0)
        score2  = image_rotate(img, img_comparison,180)
        
        score_max = max(score1,score2)
        dic = {score1:0,score2:180}
        angle = dic[score_max]
        
        if angle == 0:
            img_cut = img.copy()
            img_comparison_cut = img_comparison.copy()
        else:
            img_cut = img.copy()
            img_comparison_cut = imutils.rotate(img_comparison,angle)
        
    elif (img.shape[0]/img.shape[1]) == (img_comparison.shape[0]/img_comparison.shape[1]):
        img_cut = cv2.resize(img,(8,8),interpolation=cv2.INTER_LINEAR)
        img_comparison_cut = cv2.resize(img_comparison,(8,8),interpolation=cv2.INTER_LINEAR)
    
    elif (img.shape[0]/img.shape[1]) == (img_comparison.shape[1]/img_comparison.shape[0]):
        score1  = image_rotate_resize(img, img_comparison,90)
        score2  = image_rotate_resize(img, img_comparison,270)

        score_max = max(score1,score2)
        dic = {score1:90,score2:270}
        angle = dic[score_max]
        size = (255,255)
        img_cut = cv2.resize(img,size)
        img_comparison_mid = cv2.resize(img_comparison,size) 
        img_comparison_cut = imutils.rotate(img_comparison_mid,angle)
        
    else:
        size = (1024,1024)
        img_cut = cv2.resize(img,size,)
        img_comparison_cut = cv2.resize(img_comparison,size,)
                
    return img_cut,img_comparison_cut

def calculate_ssim(img_name, img_com_name):
    img = cv2.imread(img_name).copy()
    img_comparison = cv2.imread(img_com_name).copy()

    img_cut,img_comparison_cut = deal_image(img, img_comparison)
    
    difference = cv2.subtract(img_cut, img_comparison_cut)
    result = not np.any(difference)
    
    if result:
        score = 1.0
    else:
        grayA = cv2.cvtColor(img_cut, cv2.COLOR_BGR2GRAY)
        grayB = cv2.cvtColor(img_comparison_cut, cv2.COLOR_BGR2GRAY)
        (score, diff) = measure.compare_ssim(grayA, grayB, full=True)
        
        diff = (diff * 255).astype("uint8")
        thresh = cv2.threshold(diff,0,255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
        cnts = cv2.findContours(thresh.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)
        cnts = imutils.grab_contours(cnts)

        for c in cnts:
            (x,y,w,h) = cv2.boundingRect(c)
            cv2.rectangle(img_comparison,(x,y),(x+w,y+h),(0,0,255),2)
    cv2.imwrite(img_name+"-"+img_com_name+"-"+str(score)[0:4]+".jpg",img_comparison)
    
    return score,img,img_comparison

def calculate_show(img_name, img_com_name):
    score,img,img_comparison = calculate_ssim(img_name, img_com_name)
    print(str(score)[:4])
    
    plt.subplot(1,2,1)
    im1 = img[:,:,::-1]
    plt.imshow(im1)
    plt.title('First Image')
    plt.subplot(1,2,2)
    im2 = img_comparison[:,:,::-1]
    plt.imshow(im2)
    plt.title('Second Image')
    plt.show()
    
    return None

def get_image_testcases():
  testCases = 10
  [masked_images, masks], sample_labels = testGenerator[54]
  fig, axs = plt.subplots(nrows=testCases, figsize=(16, 2*testCases))
  for i in range(testCases):
    inputs = [masked_images[i].reshape((1,)+masked_images[i].shape), masks[i].reshape((1,)+masks[i].shape)]
    impainted_image = model.predict(inputs)
    re_impainted_image = impainted_image.reshape(impainted_image.shape[1:])
    axs[i].imshow(re_impainted_image)
    im = Image.fromarray((re_impainted_image * 255).astype(np.uint8), "RGB")
    im2 = Image.fromarray((sample_labels[i] * 255).astype(np.uint8), "RGB")
    name = "test" + str(i) + ".png"
    name2 = "result" + str(i) + ".png"
    im.save(name)
    im2.save(name2)
    #print("PSNR caso de prueba: ", psnr(io.imread(name), io.imread(name2)))

"""# Paso 10: Obtención de resultados de nuestro modelo"""

get_image_testcases()

def plotLosses(history):  
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

plotLosses(_)